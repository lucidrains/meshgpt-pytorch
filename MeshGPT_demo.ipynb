{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "!pip install  -q git+https://github.com/MarcusLoppe/meshgpt-pytorch.git"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 33,
         "metadata": {},
         "outputs": [],
         "source": [
            "import torch\n",
            "import trimesh\n",
            "import numpy as np\n",
            "import os\n",
            "import csv\n",
            "import json\n",
            "from collections import OrderedDict\n",
            "\n",
            "from meshgpt_pytorch import (\n",
            "    MeshTransformerTrainer,\n",
            "    MeshAutoencoderTrainer,\n",
            "    MeshAutoencoder,\n",
            "    MeshTransformer\n",
            ")\n",
            "from meshgpt_pytorch.data import ( \n",
            "    derive_face_edges_from_faces\n",
            ") \n",
            "\n",
            "def get_mesh(file_path): \n",
            "    mesh = trimesh.load(file_path, force='mesh') \n",
            "    vertices = mesh.vertices.tolist()\n",
            "    if \".off\" in file_path:  # ModelNet dataset\n",
            "       mesh.vertices[:, [1, 2]] = mesh.vertices[:, [2, 1]] \n",
            "       rotation_matrix = trimesh.transformations.rotation_matrix(np.radians(-90), [0, 1, 0])\n",
            "       mesh.apply_transform(rotation_matrix) \n",
            "        # Extract vertices and faces from the rotated mesh\n",
            "       vertices = mesh.vertices.tolist()\n",
            "            \n",
            "    faces = mesh.faces.tolist()\n",
            "    \n",
            "    centered_vertices = vertices - np.mean(vertices, axis=0)  \n",
            "    max_abs = np.max(np.abs(centered_vertices))\n",
            "    vertices = centered_vertices / (max_abs / 0.95)     # Limit vertices to [-0.95, 0.95]\n",
            "      \n",
            "    min_y = np.min(vertices[:, 1]) \n",
            "    difference = -0.95 - min_y \n",
            "    vertices[:, 1] += difference\n",
            "    \n",
            "    def sort_vertices(vertex):\n",
            "        return vertex[1], vertex[2], vertex[0]   \n",
            " \n",
            "    seen = OrderedDict()\n",
            "    for point in vertices: \n",
            "      key = tuple(point)\n",
            "      if key not in seen:\n",
            "        seen[key] = point\n",
            "        \n",
            "    unique_vertices =  list(seen.values()) \n",
            "    sorted_vertices = sorted(unique_vertices, key=sort_vertices)\n",
            "      \n",
            "    vertices_as_tuples = [tuple(v) for v in vertices]\n",
            "    sorted_vertices_as_tuples = [tuple(v) for v in sorted_vertices]\n",
            "\n",
            "    vertex_map = {old_index: new_index for old_index, vertex_tuple in enumerate(vertices_as_tuples) for new_index, sorted_vertex_tuple in enumerate(sorted_vertices_as_tuples) if vertex_tuple == sorted_vertex_tuple} \n",
            "    reindexed_faces = [[vertex_map[face[0]], vertex_map[face[1]], vertex_map[face[2]]] for face in faces] \n",
            "    sorted_faces = [sorted(sub_arr) for sub_arr in reindexed_faces]   \n",
            "    return np.array(sorted_vertices), np.array(sorted_faces)\n",
            " \n",
            " \n",
            "\n",
            "def augment_mesh(vertices, scale_factor):     \n",
            "    jitter_factor=0.01 \n",
            "    possible_values = np.arange(-jitter_factor, jitter_factor , 0.0005) \n",
            "    offsets = np.random.choice(possible_values, size=vertices.shape) \n",
            "    vertices = vertices + offsets   \n",
            "    \n",
            "    vertices = vertices * scale_factor \n",
            "    # To ensure that the mesh models are on the \"ground\"\n",
            "    min_y = np.min(vertices[:, 1])  \n",
            "    difference = -0.95 - min_y \n",
            "    vertices[:, 1] += difference\n",
            "    return vertices\n",
            "\n",
            "\n",
            "#load_shapenet(\"./shapenet\", \"./shapenet_csv_files\", 10, 10)   \n",
            "#Find the csv files with the labels in the ShapeNetCore.v1.zip, download at  https://huggingface.co/datasets/ShapeNet/ShapeNetCore-archive  \n",
            "def load_shapenet(directory, per_category, variations ):\n",
            "    obj_datas = []   \n",
            "    chosen_models_count = {}    \n",
            "    print(f\"per_category: {per_category} variations {variations}\")\n",
            "    \n",
            "    with open('shapenet_labels.json' , 'r') as f:\n",
            "        id_info = json.load(f) \n",
            "    \n",
            "    possible_values = np.arange(0.75, 1.0 , 0.005) \n",
            "    scale_factors = np.random.choice(possible_values, size=variations) \n",
            "    \n",
            "    for category in os.listdir(directory): \n",
            "        category_path = os.path.join(directory, category)   \n",
            "        if os.path.isdir(category_path) == False:\n",
            "            continue \n",
            "        \n",
            "        num_files_in_category = len(os.listdir(category_path))\n",
            "        print(f\"{category_path} got {num_files_in_category} files\") \n",
            "        chosen_models_count[category] = 0  \n",
            "        \n",
            "        for filename in os.listdir(category_path):\n",
            "            if filename.endswith((\".obj\", \".glb\", \".off\")):\n",
            "                file_path = os.path.join(category_path, filename)\n",
            "                \n",
            "                if chosen_models_count[category] >= per_category:\n",
            "                    break \n",
            "                if os.path.getsize(file_path) >  20 * 1024: # 20 kb limit = less then 400-600 faces\n",
            "                    continue \n",
            "                if filename[:-4] not in id_info:\n",
            "                    print(\"Unable to find id info for \", filename)\n",
            "                    continue \n",
            "                vertices, faces = get_mesh(file_path) \n",
            "                if len(faces) > 800: \n",
            "                    continue\n",
            "                \n",
            "                chosen_models_count[category] += 1  \n",
            "                textName = id_info[filename[:-4]]   \n",
            "                \n",
            "                face_edges =  derive_face_edges_from_faces(faces)  \n",
            "                for scale_factor in scale_factors: \n",
            "                    aug_vertices = augment_mesh(vertices.copy(), scale_factor)   \n",
            "                    obj_data = {\"vertices\": torch.tensor(aug_vertices.tolist(), dtype=torch.float).to(\"cuda\"), \"faces\":  torch.tensor(faces.tolist(), dtype=torch.long).to(\"cuda\"), \"face_edges\" : face_edges, \"texts\": textName }  \n",
            "                    obj_datas.append(obj_data)\n",
            "                    \n",
            "    print(\"=\"*25)\n",
            "    print(\"Chosen models count for each category:\")\n",
            "    for category, count in chosen_models_count.items():\n",
            "        print(f\"{category}: {count}\") \n",
            "    total_chosen_models = sum(chosen_models_count.values())\n",
            "    print(f\"Total number of chosen models: {total_chosen_models}\")\n",
            "    return obj_datas\n",
            "\n",
            "  \n",
            "   \n",
            "def load_filename(directory, variations):\n",
            "    obj_datas = []    \n",
            "    possible_values = np.arange(0.75, 1.0 , 0.005) \n",
            "    scale_factors = np.random.choice(possible_values, size=variations) \n",
            "    \n",
            "    for filename in os.listdir(directory):\n",
            "        if filename.endswith((\".obj\", \".glb\", \".off\")): \n",
            "            file_path = os.path.join(directory, filename) \n",
            "            vertices, faces = get_mesh(file_path)  \n",
            "            \n",
            "            faces = torch.tensor(faces.tolist(), dtype=torch.long).to(\"cuda\")\n",
            "            face_edges =  derive_face_edges_from_faces(faces)  \n",
            "            texts, ext = os.path.splitext(filename)     \n",
            "            \n",
            "            for scale_factor in scale_factors: \n",
            "                aug_vertices = augment_mesh(vertices.copy(), scale_factor)  \n",
            "                obj_data = {\"vertices\": torch.tensor(aug_vertices.tolist(), dtype=torch.float).to(\"cuda\"), \"faces\":  faces, \"face_edges\" : face_edges, \"texts\": texts } \n",
            "                obj_datas.append(obj_data)\n",
            "                     \n",
            "    print(f\"[create_mesh_dataset] Returning {len(obj_data)} meshes\")\n",
            "    return obj_datas"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import gzip,json\n",
            "from tqdm import tqdm\n",
            "import pandas as pd\n",
            "\n",
            "# Instruction to download objverse meshes: https://github.com/MarcusLoppe/Objaverse-downloader/tree/main\n",
            "def load_objverse(directory, variations ):\n",
            "    obj_datas = []     \n",
            "    id_info = {}   \n",
            "    pali_captions = pd.read_csv('.\\pali_captions.csv', sep=';') # https://github.com/google-deepmind/objaverse_annotations/blob/main/pali_captions.csv\n",
            "    pali_captions_dict = pali_captions.set_index(\"object_uid\").to_dict()[\"top_aggregate_caption\"]  \n",
            "        \n",
            "    possible_values = np.arange(0.75, 1.0) \n",
            "    scale_factors = np.random.choice(possible_values, size=variations) \n",
            "    \n",
            "    for folder in os.listdir(directory):  \n",
            "        full_folder_path = os.path.join(directory, folder)   \n",
            "        if os.path.isdir(full_folder_path) == False:\n",
            "            continue    \n",
            "         \n",
            "        for filename in tqdm(os.listdir(full_folder_path)):  \n",
            "            if filename.endswith((\".obj\", \".glb\", \".off\")):\n",
            "                file_path = os.path.join(full_folder_path, filename)\n",
            "                kb = os.path.getsize(file_path)  / 1024 \n",
            "                if kb < 1 or kb > 30:\n",
            "                    continue\n",
            "                  \n",
            "                if filename[:-4] not in pali_captions_dict: \n",
            "                    continue   \n",
            "                textName =  pali_captions_dict[filename[:-4]]\n",
            "                try:    \n",
            "                    vertices, faces = get_mesh(file_path)   \n",
            "                except Exception as e:\n",
            "                    continue\n",
            "                \n",
            "                if len(faces) > 250 or len(faces) < 50: \n",
            "                    continue\n",
            "                \n",
            "                faces = torch.tensor(faces.tolist(), dtype=torch.long).to(\"cuda\")\n",
            "                face_edges = derive_face_edges_from_faces(faces)   \n",
            "                for scale_factor in scale_factors: \n",
            "                    aug_vertices = augment_mesh(vertices.copy(), scale_factor)   \n",
            "                    obj_data = {\"filename\": filename, \"vertices\": torch.tensor(aug_vertices.tolist(), dtype=torch.float).to(\"cuda\"), \"faces\":  faces, \"face_edges\" : face_edges, \"texts\": textName }   \n",
            "                    obj_datas.append(obj_data)  \n",
            "    return obj_datas"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from pathlib import Path \n",
            "import gc     \n",
            "import os\n",
            "from meshgpt_pytorch import MeshDataset \n",
            " \n",
            "project_name = \"demo_mesh\" \n",
            "\n",
            "working_dir = f'.\\{project_name}'\n",
            "\n",
            "working_dir = Path(working_dir)\n",
            "working_dir.mkdir(exist_ok = True, parents = True)\n",
            "dataset_path = working_dir / (project_name + \".npz\")\n",
            " \n",
            "if not os.path.isfile(dataset_path):\n",
            "    data = load_filename(\"./demo_mesh\",50)  \n",
            "    dataset = MeshDataset(data) \n",
            "    dataset.generate_face_edges()  \n",
            "    dataset.save(dataset_path)\n",
            " \n",
            "dataset = MeshDataset.load(dataset_path) \n",
            "print(dataset.data[0].keys())"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Inspect imported meshes (optional)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from pathlib import Path\n",
            " \n",
            "folder = working_dir / f'renders' \n",
            "obj_file_path = Path(folder)\n",
            "obj_file_path.mkdir(exist_ok = True, parents = True)\n",
            "   \n",
            "all_vertices = []\n",
            "all_faces = []\n",
            "vertex_offset = 0\n",
            "translation_distance = 0.5  \n",
            "\n",
            "for r, item in enumerate(data): \n",
            "    vertices_copy =  np.copy(item['vertices'])\n",
            "    vertices_copy += translation_distance * (r / 0.2 - 1) \n",
            "    \n",
            "    for vert in vertices_copy:\n",
            "        vertex = vert.to('cpu')\n",
            "        all_vertices.append(f\"v {float(vertex[0])}  {float(vertex[1])}  {float(vertex[2])}\\n\") \n",
            "    for face in item['faces']:\n",
            "        all_faces.append(f\"f {face[0]+1+ vertex_offset} {face[1]+ 1+vertex_offset} {face[2]+ 1+vertex_offset}\\n\")  \n",
            "    vertex_offset = len(all_vertices)\n",
            " \n",
            "obj_file_content = \"\".join(all_vertices) + \"\".join(all_faces)\n",
            " \n",
            "obj_file_path = f'{folder}/3d_models_inspect.obj' \n",
            "with open(obj_file_path, \"w\") as file:\n",
            "    file.write(obj_file_content)    \n",
            "    "
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Train!"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "autoencoder = MeshAutoencoder(      \n",
            "        decoder_dims_through_depth =  (128,) * 6 + (192,) * 12 + (256,) * 24 + (384,) * 6,   \n",
            "        codebook_size = 2048,  # Smaller vocab size will speed up the transformer training, however if you are training on meshes more then 250 triangle, I'd advice to use 16384 codebook size\n",
            "        dim_codebook = 192,  \n",
            "        dim_area_embed = 16,\n",
            "        dim_coor_embed = 16, \n",
            "        dim_normal_embed = 16,\n",
            "        dim_angle_embed = 8,\n",
            "    \n",
            "    attn_decoder_depth  = 4,\n",
            "    attn_encoder_depth = 2\n",
            ").to(\"cuda\")     \n",
            "total_params = sum(p.numel() for p in autoencoder.parameters()) \n",
            "total_params = f\"{total_params / 1000000:.1f}M\"\n",
            "print(f\"Total parameters: {total_params}\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Have at least 400-2000 items in the dataset, use this to multiply the dataset**  "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "dataset.data = [dict(d) for d in dataset.data] * 10\n",
            "print(len(dataset.data))"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "*Load previous saved model if you had to restart session*"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pkg = torch.load(str(f'{working_dir}\\mesh-encoder_{project_name}.pt')) \n",
            "autoencoder.load_state_dict(pkg['model'])\n",
            "for param in autoencoder.parameters():\n",
            "    param.requires_grad = True"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Train to about 0.3 loss if you are using a small dataset**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "batch_size=16 # The batch size should be max 64.\n",
            "grad_accum_every = 4\n",
            "# So set the maximal batch size (max 64) that your VRAM can handle and then use grad_accum_every to create a effective batch size of 64, e.g  16 * 4 = 64\n",
            "learning_rate = 1e-3 # Start with 1e-3 then at staggnation around 0.35, you can lower it to 1e-4.\n",
            "\n",
            "autoencoder.commit_loss_weight = 0.2 # Set dependant on the dataset size, on smaller datasets, 0.1 is fine, otherwise try from 0.25 to 0.4.\n",
            "autoencoder_trainer = MeshAutoencoderTrainer(model =autoencoder ,warmup_steps = 10, dataset = dataset, num_train_steps=100,\n",
            "                                             batch_size=batch_size,\n",
            "                                             grad_accum_every = grad_accum_every,\n",
            "                                             learning_rate = learning_rate,\n",
            "                                             checkpoint_every_epoch=1) \n",
            "loss = autoencoder_trainer.train(480,stop_at_loss = 0.2, diplay_graph= True)        "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "autoencoder_trainer.save(f'{working_dir}\\mesh-encoder_{project_name}.pt')   "
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Inspect how the autoencoder can encode and then provide the decoder with the codes to reconstruct the mesh"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import torch\n",
            "import random\n",
            "from tqdm import tqdm \n",
            "from meshgpt_pytorch import mesh_render \n",
            "\n",
            "min_mse, max_mse = float('inf'), float('-inf')\n",
            "min_coords, min_orgs, max_coords, max_orgs = None, None, None, None\n",
            "random_samples, random_samples_pred, all_random_samples = [], [], []\n",
            "total_mse, sample_size = 0.0, 200\n",
            "\n",
            "random.shuffle(dataset.data)\n",
            "\n",
            "for item in tqdm(dataset.data[:sample_size]):\n",
            "    codes = autoencoder.tokenize(vertices=item['vertices'], faces=item['faces'], face_edges=item['face_edges']) \n",
            "    \n",
            "    codes = codes.flatten().unsqueeze(0)\n",
            "    codes = codes[:, :codes.shape[-1] // autoencoder.num_quantizers * autoencoder.num_quantizers] \n",
            " \n",
            "    coords, mask = autoencoder.decode_from_codes_to_faces(codes)\n",
            "    orgs = item['vertices'][item['faces']].unsqueeze(0)\n",
            "\n",
            "    mse = torch.mean((orgs.view(-1, 3).cpu() - coords.view(-1, 3).cpu())**2)\n",
            "    total_mse += mse \n",
            "\n",
            "    if mse < min_mse: min_mse, min_coords, min_orgs = mse, coords, orgs\n",
            "    if mse > max_mse: max_mse, max_coords, max_orgs = mse, coords, orgs\n",
            " \n",
            "    if len(random_samples) <= 30:\n",
            "        random_samples.append(coords)\n",
            "        random_samples_pred.append(orgs)\n",
            "    else:\n",
            "        all_random_samples.extend([random_samples_pred, random_samples])\n",
            "        random_samples, random_samples_pred = [], []\n",
            "\n",
            "print(f'MSE AVG: {total_mse / sample_size:.10f}, Min: {min_mse:.10f}, Max: {max_mse:.10f}')    \n",
            "mesh_render.combind_mesh_with_rows(f'{working_dir}\\mse_rows.obj', all_random_samples)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Training & fine-tuning\n",
            "\n",
            "**Pre-train:** Train the transformer on the full dataset with all the augmentations, the longer / more epochs will create a more robust model.<br/>\n",
            "\n",
            "**Fine-tune:** Since it will take a long time to train on all the possible augmentations of the meshes, I recommend that you remove all the augmentations so you are left with x1 model per mesh.<br/>\n",
            "Below is the function **filter_dataset** that will return a single copy of each mesh.<br/>\n",
            "The function can also check for duplicate labels, this may speed up the fine-tuning process (not recommanded) however this most likely will remove it's ability for novel mesh generation."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import gc  \n",
            "torch.cuda.empty_cache()\n",
            "gc.collect()   \n",
            "max_seq = max(len(d[\"faces\"]) for d in dataset if \"faces\" in d)  * (autoencoder.num_vertices_per_face * autoencoder.num_quantizers) \n",
            "print(\"Max token sequence:\" , max_seq)  \n",
            "\n",
            "# GPT2-Small model\n",
            "transformer = MeshTransformer(\n",
            "    autoencoder,\n",
            "    dim = 768,\n",
            "    coarse_pre_gateloop_depth = 3,  \n",
            "    fine_pre_gateloop_depth= 3,  \n",
            "    attn_depth = 12,  \n",
            "    attn_heads = 12,  \n",
            "    max_seq_len = max_seq, \n",
            "    condition_on_text = True, \n",
            "    gateloop_use_heinsen = False,\n",
            "    dropout  = 0.0,\n",
            "    text_condition_model_types = \"bge\", \n",
            "    text_condition_cond_drop_prob = 0.0\n",
            ") \n",
            "\n",
            "total_params = sum(p.numel() for p in transformer.decoder.parameters())\n",
            "total_params = f\"{total_params / 1000000:.1f}M\"\n",
            "print(f\"Decoder total parameters: {total_params}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def filter_dataset(dataset, unique_labels = False):\n",
            "    unique_dicts = []\n",
            "    unique_tensors = set()\n",
            "    texts = set()\n",
            "    for d in dataset.data:\n",
            "        tensor = d[\"faces\"]\n",
            "        tensor_tuple = tuple(tensor.cpu().numpy().flatten())\n",
            "        if unique_labels and d['texts'] in texts:\n",
            "            continue\n",
            "        if tensor_tuple not in unique_tensors:\n",
            "            unique_tensors.add(tensor_tuple)\n",
            "            unique_dicts.append(d)\n",
            "            texts.add(d['texts'])\n",
            "    return unique_dicts \n",
            "#dataset.data = filter_dataset(dataset.data, unique_labels = False)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## **Required!**, embed the text and run generate_codes to save 4-96 GB VRAM (dependant on dataset) ##\n",
            "\n",
            "**If you don't;** <br>\n",
            "During each during each training step the autoencoder will generate the codes and the text encoder will embed the text.\n",
            "<br>\n",
            "After these fields are generate: **they will be deleted and next time it generates the code again:**<br>\n",
            "\n",
            "This is due to the dataloaders nature, it writes this information to a temporary COPY of the dataset\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "labels = list(set(item[\"texts\"] for item in dataset.data))\n",
            "dataset.embed_texts(transformer, batch_size = 25)\n",
            "dataset.generate_codes(autoencoder, batch_size = 50)\n",
            "print(dataset.data[0].keys())"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "*Load previous saved model if you had to restart session*"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pkg = torch.load(str(f'{working_dir}\\mesh-transformer_{project_name}.pt')) \n",
            "transformer.load_state_dict(pkg['model'])"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Train to about 0.0001 loss (or less) if you are using a small dataset**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "batch_size = 4 # Max 64\n",
            "grad_accum_every = 16\n",
            "\n",
            "# Set the maximal batch size (max 64) that your VRAM can handle and then use grad_accum_every to create a effective batch size of 64, e.g  4 * 16 = 64\n",
            "learning_rate = 1e-2 # Start training with the learning rate at 1e-2 then lower it to 1e-3 at stagnation or at 0.5 loss.\n",
            "\n",
            "trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,num_train_steps=100, dataset = dataset,\n",
            "                                 grad_accum_every=grad_accum_every,\n",
            "                                 learning_rate = learning_rate,\n",
            "                                 batch_size=batch_size,\n",
            "                                 checkpoint_every_epoch = 1,\n",
            "                                 # FP16 training, it doesn't speed up very much but can increase the batch size which will in turn speed up the training.\n",
            "                                 # However it might cause nan after a while.\n",
            "                                 # accelerator_kwargs = {\"mixed_precision\" : \"fp16\"}, optimizer_kwargs = { \"eps\": 1e-7} \n",
            "                                 )\n",
            "loss = trainer.train(300, stop_at_loss = 0.005)  "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "trainer.save(f'{working_dir}\\mesh-transformer_{project_name}.pt')   "
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Generate and view mesh"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Using only text**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            " \n",
            "from meshgpt_pytorch import mesh_render \n",
            "from pathlib import Path\n",
            " \n",
            "folder = working_dir / 'renders'\n",
            "obj_file_path = Path(folder)\n",
            "obj_file_path.mkdir(exist_ok = True, parents = True)  \n",
            " \n",
            "text_coords = [] \n",
            "for text in labels[:10]:\n",
            "    print(f\"Generating {text}\") \n",
            "    text_coords.append(transformer.generate(texts = [text],  temperature = 0.0))   \n",
            "    \n",
            "mesh_render.save_rendering(f'{folder}/3d_models_all.obj', text_coords)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Text + prompt of tokens**"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Prompt with 10% of codes/tokens**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from pathlib import Path \n",
            "from meshgpt_pytorch import mesh_render \n",
            "folder = working_dir / f'renders/text+codes'\n",
            "obj_file_path = Path(folder)\n",
            "obj_file_path.mkdir(exist_ok = True, parents = True)  \n",
            "\n",
            "token_length_procent = 0.10 \n",
            "codes = []\n",
            "texts = []\n",
            "for label in labels:\n",
            "    for item in dataset.data: \n",
            "        if item['texts'] == label:\n",
            "            tokens = autoencoder.tokenize(\n",
            "                vertices = item['vertices'],\n",
            "                faces = item['faces'],\n",
            "                face_edges = item['face_edges']\n",
            "            ) \n",
            "            num_tokens = int(tokens.shape[0] * token_length_procent)  \n",
            "            texts.append(item['texts']) \n",
            "            codes.append(tokens.flatten()[:num_tokens].unsqueeze(0))  \n",
            "            break\n",
            "        \n",
            "coords = []  \n",
            "for text, prompt in zip(texts, codes): \n",
            "    print(f\"Generating {text} with {prompt.shape[1]} tokens\") \n",
            "    coords.append(transformer.generate(texts = [text],  prompt = prompt, temperature = 0) )    \n",
            "      \n",
            "mesh_render.save_rendering(f'{folder}/text+prompt_{token_length_procent*100}.obj', coords)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Prompt with 0% to 80% of tokens**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from pathlib import Path\n",
            "from meshgpt_pytorch import mesh_render \n",
            " \n",
            "folder = working_dir / f'renders/text+codes_rows'\n",
            "obj_file_path = Path(folder)\n",
            "obj_file_path.mkdir(exist_ok = True, parents = True)   \n",
            "\n",
            "mesh_rows = []\n",
            "for token_length_procent in np.arange(0, 0.8, 0.1):\n",
            "    codes = []\n",
            "    texts = []\n",
            "    for label in labels:\n",
            "        for item in dataset.data: \n",
            "            if item['texts'] == label:\n",
            "                tokens = autoencoder.tokenize(\n",
            "                    vertices = item['vertices'],\n",
            "                    faces = item['faces'],\n",
            "                    face_edges = item['face_edges']\n",
            "                ) \n",
            "                num_tokens = int(tokens.shape[0] * token_length_procent) \n",
            "                \n",
            "                texts.append(item['texts']) \n",
            "                codes.append(tokens.flatten()[:num_tokens].unsqueeze(0))  \n",
            "                break\n",
            "            \n",
            "    coords = []   \n",
            "    for text, prompt in zip(texts, codes):  \n",
            "        print(f\"Generating {text} with {prompt.shape[1]} tokens\") \n",
            "        coords.append(transformer.generate(texts = [text],  prompt = prompt, temperature = 0)) \n",
            "         \n",
            "    mesh_rows.append(coords)  \n",
            "    \n",
            "mesh_render.save_rendering(f'{folder}/all.obj', mesh_rows)\n",
            " "
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Just some testing for text embedding similarity**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import numpy as np \n",
            "texts = list(labels)\n",
            "vectors = [transformer.conditioner.text_models[0].embed_text([text], return_text_encodings = False).cpu().flatten() for text in texts]\n",
            " \n",
            "max_label_length = max(len(text) for text in texts)\n",
            " \n",
            "# Print the table header\n",
            "print(f\"{'Text':<{max_label_length}} |\", end=\" \")\n",
            "for text in texts:\n",
            "    print(f\"{text:<{max_label_length}} |\", end=\" \")\n",
            "print()\n",
            "\n",
            "# Print the similarity matrix as a table with fixed-length columns\n",
            "for i in range(len(texts)):\n",
            "    print(f\"{texts[i]:<{max_label_length}} |\", end=\" \")\n",
            "    for j in range(len(texts)):\n",
            "        # Encode the texts and calculate cosine similarity manually\n",
            "        vector_i = vectors[i]\n",
            "        vector_j = vectors[j]\n",
            "        \n",
            "        dot_product = torch.sum(vector_i * vector_j)\n",
            "        norm_vector1 = torch.norm(vector_i)\n",
            "        norm_vector2 = torch.norm(vector_j)\n",
            "        similarity_score = dot_product / (norm_vector1 * norm_vector2)\n",
            "        \n",
            "        # Print with fixed-length columns\n",
            "        print(f\"{similarity_score.item():<{max_label_length}.4f} |\", end=\" \")\n",
            "    print()"
         ]
      }
   ],
   "metadata": {
      "kaggle": {
         "accelerator": "gpu",
         "dataSources": [],
         "dockerImageVersionId": 30627,
         "isGpuEnabled": true,
         "isInternetEnabled": true,
         "language": "python",
         "sourceType": "notebook"
      },
      "kernelspec": {
         "display_name": "Python 3",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.5"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 4
}
